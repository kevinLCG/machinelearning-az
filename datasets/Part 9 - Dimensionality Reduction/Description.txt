¿Recuerdas cómo en la Parte 3 - Clasificación, trabajamos con datasets compuestos únicamente de dos variables independientes? Básicamente lo hicimos así por dos motivos:

1.-	Ya que necesitábamos dos variables para visualizar mejor cómo funcionan los algoritmos de Machine 
    Learning  (para poder representar las regiones de decisión y la frontera de predicción de cada modelo).

2.- Ya que dado cualquier número de variables independientes en un problema, normalmente se puede 
    acabar con únicamente dos variables independientes a través de técnicas de reducción de la dimensión.


Existen dos tipos de técnicas para reducir la dimensión de un dataset:

  - Selección de características
  - Extracción de características


La "selección de características" incluye técnicas como:
  - Eliminación hacia atrás
  - Selección directa
  - Eliminación bidimensional
  - Comparación de scores

y muchas más. Todas ellas las vimos ya en la parte de Regresión.

En esta parte cubriremos la Extracción de características que incluyen:

  - Análisis de Componentes Principales (ACP)
  - Análisis Discriminante Lineal (LDA)
  - Kernel PCA
  - Análisis Discriminante Cuadrático (QDA)